{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"train.csv\", low_memory=False)\n",
    "data.drop_duplicates() # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING OF STORIES\n",
    "\n",
    "\n",
    "# Map the 'ImageData.style.stories.summary.label' column to numeric values\n",
    "story_mapping = {\n",
    "    '1_story': 1,\n",
    "    '1.5_stories': 1.5,\n",
    "    '2_stories': 2,\n",
    "    '2.5_stories': 2.5,\n",
    "    '3_stories_or_more': 3\n",
    "}\n",
    "\n",
    "\n",
    "# print(data['ImageData.style.stories.summary.label'].unique())\n",
    "data['ImageData.style.stories.summary.label'] = data['ImageData.style.stories.summary.label'].map(story_mapping)\n",
    "print(data['ImageData.style.stories.summary.label'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Structure.YearBuilt'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replace edge case \n",
    "# data['Structure.YearBuilt'] = data['Structure.YearBuilt'].replace(190., np.nan)\n",
    "\n",
    "# # convert 0 to nan on Structure.YearBuilt to remove 0 values\n",
    "# data['Structure.YearBuilt'] = data['Structure.YearBuilt'].replace(0, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data segmentation\n",
    "columnes_numeriques = data.select_dtypes(include=['number'])  # Selecciona columnes numèriques\n",
    "# add ListingId to the numeric columns\n",
    "columnes_numeriques = pd.concat([data['Listing.ListingId'], columnes_numeriques], axis=1)\n",
    "\n",
    "columnes_categoriques = data.select_dtypes(exclude=['number'])  # Selecciona columnes no numèriques (categòriques)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Treballar amb cada columna numèrica una a una\n",
    "for columna in columnes_numeriques.columns:\n",
    "    if(columna in ['Listing.ListingId', 'Listing.Price.ClosePrice']): continue\n",
    "    # Calcular Q1, Q3 i IQR per a la columna actual (ignorant els NaN)\n",
    "    Q1 = columnes_numeriques[columna].quantile(0.1)\n",
    "    Q3 = columnes_numeriques[columna].quantile(0.9)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Límits per identificar outliers\n",
    "    lower_bound = Q1 - 2 * IQR\n",
    "    upper_bound = Q3 + 2 * IQR\n",
    "    \n",
    "    # Filtrar files dins dels límits, mantenint files amb NaN\n",
    "    mask = (columnes_numeriques[columna].isna()) | \\\n",
    "        ((columnes_numeriques[columna] >= lower_bound) & (columnes_numeriques[columna] <= upper_bound))\n",
    "    \n",
    "    columnes_numeriques = columnes_numeriques[mask]\n",
    "\n",
    "columnes_numeriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canviar els NaN per la mitjana de la columna\n",
    "columnes_numeriques = columnes_numeriques.apply(lambda col: col.fillna(col.mean(skipna=True)) if col.dtype in ['float64', 'int64'] else col)\n",
    "columnes_numeriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize all numeric columns except ListingId and ClosePrice\n",
    "# scaler = MinMaxScaler()\n",
    "# columnes_numeriques = pd.DataFrame(scaler.fit_transform(columnes_numeriques), columns=columnes_numeriques.columns)\n",
    "# columnes_numeriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnes_categoriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every column see how many unique values it has and its count\n",
    "for col in columnes_categoriques.columns:\n",
    "    print(columnes_categoriques[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INUTILS o REDUNDANTS -> BORRAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some fine tuning\n",
    "# drop columns with too many unique values\n",
    "# drop Listing.ListingId\n",
    "columnes_categoriques = columnes_categoriques.drop('Listing.ListingId', axis=1)\n",
    "\n",
    "# drop postalcodeplus4\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.PostalCodePlus4', axis=1)\n",
    "\n",
    "# drop location number Location.Address.StreetNumber\n",
    "# numero independent del preu\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.StreetNumber', axis=1)\n",
    "\n",
    "# DROP Location.Address.StateOrProvince\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.StateOrProvince', axis=1)\n",
    "\n",
    "# DROP Location.Address.UnparsedAddress\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.UnparsedAddress', axis=1)\n",
    "\n",
    "# DROP Location.Address.StreetDirectionSuffix\n",
    "# nombre d'ocurrencies no es prou gran per cada categoria\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.StreetDirectionSuffix', axis=1)\n",
    "\n",
    "# DROP UnitNumber\n",
    "# nombre d'ocurrencies no es prou gran per cada categoria\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.UnitNumber', axis=1)\n",
    "\n",
    "# DROP Location.Address.StreetSuffix\n",
    "# no es rellevant\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.StreetSuffix', axis=1)\n",
    "\n",
    "#drop Location.Address.StreetDirectionPrefix\n",
    "# no es rellevant\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.StreetDirectionPrefix', axis=1)\n",
    "\n",
    "# DROP Location.Address.StreetName\n",
    "# nombre d'ocurrencies no es prou gran per cada categoria\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.StreetName', axis=1)\n",
    "\n",
    "# DROP Location.Area.SubdivisionName\n",
    "# no es rellevant\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Area.SubdivisionName', axis=1)\n",
    "\n",
    "# DROP Location.School.HighSchoolDistrict\n",
    "# no es rellevant\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.School.HighSchoolDistrict', axis=1)\n",
    "\n",
    "#DRop postalcode\n",
    "# no es rellevant\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.PostalCode', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHUNGAS BORRAR (TEMPORAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP  \n",
    "columnes_categoriques = columnes_categoriques.drop('UnitTypes.UnitTypeType', axis=1)\n",
    "columnes_categoriques = columnes_categoriques.drop('Structure.ParkingFeatures', axis=1)\n",
    "columnes_categoriques = columnes_categoriques.drop('Structure.Heating', axis=1)\n",
    "columnes_categoriques = columnes_categoriques.drop('Structure.Cooling', axis=1)\n",
    "columnes_categoriques = columnes_categoriques.drop('Structure.Basement', axis=1)\n",
    "# # columnes_categoriques = columnes_categoriques.drop('ImageData.room_type_reso.results', axis=1)\n",
    "# columnes_categoriques = columnes_categoriques.drop('ImageData.features_reso.results', axis=1)\n",
    "columnes_categoriques = columnes_categoriques.drop('Characteristics.LotFeatures', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raras borrar (temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all census\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.CensusBlock', axis=1)\n",
    "columnes_categoriques = columnes_categoriques.drop('Location.Address.CensusTract', axis=1)\n",
    "\n",
    "# drop Listing.Date.CloseDate\n",
    "columnes_categoriques = columnes_categoriques.drop('Listing.Dates.CloseDate', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnes_categoriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every column see how many unique values it has and its count\n",
    "for col in columnes_categoriques.columns:\n",
    "    print(columnes_categoriques[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chungas start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnes_categoriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def safe_eval(val):\n",
    "    if pd.isna(val):  # Check for NaN\n",
    "        return val\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except ValueError:\n",
    "        return val\n",
    "\n",
    "def cleanColumn(column):\n",
    "    # Eliminar les categories que apareixen menys d'un 5% de les vegades\n",
    "    value_counts = column.value_counts()\n",
    "    threshold = value_counts.max() * 0.05\n",
    "    to_replace = value_counts[value_counts < threshold].index\n",
    "    return column.replace(to_replace, 'Others')\n",
    "\n",
    "columnes_categoriques2 = columnes_categoriques.copy()\n",
    "\n",
    "for columna in columnes_categoriques2.columns:\n",
    "    if(columna in ['UnitTypes.UnitTypeType',\n",
    "        'Structure.ParkingFeatures',\n",
    "        'Structure.Heating',\n",
    "        'Structure.Cooling',\n",
    "        'Structure.Basement',\n",
    "        'ImageData.room_type_reso.results',\n",
    "        'ImageData.features_reso.results',\n",
    "        'Characteristics.LotFeatures']):\n",
    "        # es chunga\n",
    "        # # Explode the column with\n",
    "        # Assuming 'data' is the DataFrame and 'ImageData.features_reso.results' is the column with array values\n",
    "        columnes_categoriques[columna] = columnes_categoriques[columna].map(safe_eval)\n",
    "\n",
    "        data_exploded = columnes_categoriques[columna].explode(columna)\n",
    "        # dades explotades donat que era chunga. ara cal netejar-les\n",
    "    \n",
    "        # # apply cleanColumn to the column\n",
    "        data_exploded = cleanColumn(data_exploded)\n",
    "\n",
    "        data_exploded = pd.get_dummies(data_exploded, prefix=columna, prefix_sep='=')\n",
    "        \n",
    "        # # condense the dummies by summing booleans column-wise\n",
    "        data_exploded = data_exploded.groupby(data_exploded.columns, axis=1).sum()\n",
    "        # # merge the dummies back to the original dataframe\n",
    "        \n",
    "        columnes_categoriques = columnes_categoriques.merge(data_exploded, left_index=True, right_index=True)\n",
    "        columnes_categoriques.drop(columna, axis=1, inplace=True)\n",
    "    else: \n",
    "        # apply cleanColumn to the column\n",
    "        columnes_categoriques[columna] = cleanColumn(columnes_categoriques[columna])\n",
    "        novesdummiessimple = pd.get_dummies(columnes_categoriques[columna], drop_first=True)\n",
    "        columnes_categoriques = pd.concat([columnes_categoriques, novesdummiessimple], axis=1)\n",
    "        columnes_categoriques.drop(columna, axis=1, inplace=True)\n",
    "    \n",
    "    print('iteracio:', columna)\n",
    "\n",
    "\n",
    "columnes_categoriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every column see how many unique values it has and its count\n",
    "for col in columnes_categoriques.columns:\n",
    "    print(columnes_categoriques[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ListingId to the categorical columns\n",
    "columnes_categoriques = pd.concat([data['Listing.ListingId'], columnes_categoriques], axis=1)\n",
    "columnes_categoriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural join of the two dataframes\n",
    "data_final = pd.merge(columnes_numeriques, columnes_categoriques, on='Listing.ListingId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv('columnes_categoriques_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnes_numeriques.to_csv('columnes_numeriques.csv', index=False)\n",
    "columnes_categoriques.to_csv('columnes_categoriques.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
