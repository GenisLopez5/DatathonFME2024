{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creació dels arrays de test i fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "Train = pd.read_csv(\"columnes_numeriques.csv\", low_memory=False)\n",
    "y_train = Train.iloc[:, 12] # preu dels train\n",
    "x_train = Train.drop(Train.columns[12], axis=1) # valors excepte el train \n",
    "\n",
    "Test = pd.read_csv(\"columnes_numeriques.csv\", low_memory=False)\n",
    "x_test = Test.drop(Test.columns[12], axis=1) # valors except el preu del test\n",
    "y_test = Test.iloc[:, 12] #preu dels train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funció per comprovar la precisió de les prediccions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_precision_MAE(prediccio, resultat):\n",
    "    parelles = zip(prediccio, resultat)\n",
    "    sumatori = 0\n",
    "    for pred, resul in parelles:\n",
    "        sumatori += abs(pred-resul)\n",
    "    return sumatori / len(prediccio)\n",
    "\n",
    "def check_precision_MSE(prediccio, resultat):\n",
    "    parelles = zip(prediccio, resultat)\n",
    "    sumatori = 0\n",
    "    for pred, resul in parelles:\n",
    "        sumatori += (pred - resul)**2\n",
    "    return sumatori / len(prediccio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_precision_MSE([1,1,2,2,3,3], [1,1,2,2,3,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Rergession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mean square error és: 0.00821144690729602\n",
      "La mean absolute error és 0.00821144690729602\n"
     ]
    }
   ],
   "source": [
    "modelLR = LinearRegression()\n",
    "modelLR.fit(x_train, y_train)  # Entrena el model\n",
    "y_pred = modelLR.predict(x_test)  # Prediu el resultat\n",
    "\n",
    "print(\"La mean square error és: \" + str(check_precision_MSE(y_pred, y_test)))\n",
    "print(\"La mean absolute error és \" + str(check_precision_MSE(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [83351, 105104]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m X_poly \u001b[38;5;241m=\u001b[39m poly\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[0;32m      4\u001b[0m modelPL \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodelPL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_poly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m modelPL\u001b[38;5;241m.\u001b[39mpredict(poly\u001b[38;5;241m.\u001b[39mtransform(x_test))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLa mean square error és: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(check_precision_MSE(y_pred, y_test)))\n",
      "File \u001b[1;32mc:\\Users\\Daniel G\\anaconda3\\envs\\datathon\\lib\\site-packages\\sklearn\\linear_model\\_base.py:684\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    680\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    682\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 684\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    688\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    689\u001b[0m     sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    690\u001b[0m )\n\u001b[0;32m    692\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[0;32m    693\u001b[0m     X,\n\u001b[0;32m    694\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    698\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    699\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Daniel G\\anaconda3\\envs\\datathon\\lib\\site-packages\\sklearn\\base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    594\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    597\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Daniel G\\anaconda3\\envs\\datathon\\lib\\site-packages\\sklearn\\utils\\validation.py:1092\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1074\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1075\u001b[0m     X,\n\u001b[0;32m   1076\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1088\u001b[0m )\n\u001b[0;32m   1090\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1092\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\Daniel G\\anaconda3\\envs\\datathon\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [83351, 105104]"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(x_train)\n",
    "\n",
    "modelPL = LinearRegression()\n",
    "modelPL.fit(X_poly, y_train)\n",
    "y_pred = modelPL.predict(poly.transform(x_test))\n",
    "\n",
    "print(\"La mean square error és: \" + str(check_precision_MSE(y_pred, y_test)))\n",
    "print(\"La mean absolute error és \" + str(check_precision_MSE(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mean square error és: 0.0001263207089428229\n",
      "La mean absolute error és0.0001263207089428229\n"
     ]
    }
   ],
   "source": [
    "modelDTR = DecisionTreeRegressor(max_depth=10)\n",
    "modelDTR.fit(x_train, y_train)\n",
    "y_pred = modelDTR.predict(x_test)\n",
    "\n",
    "print(\"La mean square error és: \" + str(check_precision_MSE(y_pred, y_test)))\n",
    "print(\"La mean absolute error és\" + str(check_precision_MSE(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mean square error és: 2.3315076816655656e-05\n",
      "La mean absolute error és 2.3315076816655656e-05\n"
     ]
    }
   ],
   "source": [
    "modelRF = RandomForestRegressor(n_estimators=100)\n",
    "modelRF.fit(x_train, y_train)\n",
    "y_pred = modelRF.predict(x_test)\n",
    "\n",
    "print(\"La mean square error és: \" + str(check_precision_MSE(y_pred, y_test)))\n",
    "print(\"La mean absolute error és \" + str(check_precision_MSE(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nearest Neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La mean square error és: 0.0002446475760792689\n",
      "La mean absolute error és0.0002446475760792689\n"
     ]
    }
   ],
   "source": [
    "model = KNeighborsRegressor(n_neighbors=10)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"La mean square error és: \" + str(check_precision_MSE(y_pred, y_test)))\n",
    "print(\"La mean absolute error és \" + str(check_precision_MSE(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2605/2605 [==============================] - 3s 934us/step - loss: 0.0023\n",
      "Epoch 2/100\n",
      "2605/2605 [==============================] - 2s 950us/step - loss: 3.2329e-04\n",
      "Epoch 3/100\n",
      "2605/2605 [==============================] - 2s 937us/step - loss: 3.2009e-04\n",
      "Epoch 4/100\n",
      "2605/2605 [==============================] - 2s 937us/step - loss: 3.0467e-04\n",
      "Epoch 5/100\n",
      "2605/2605 [==============================] - 3s 978us/step - loss: 2.9592e-04\n",
      "Epoch 6/100\n",
      "2605/2605 [==============================] - 2s 937us/step - loss: 2.8421e-04\n",
      "Epoch 7/100\n",
      "2605/2605 [==============================] - 2s 947us/step - loss: 2.7886e-04\n",
      "Epoch 8/100\n",
      "2605/2605 [==============================] - 2s 953us/step - loss: 2.6982e-04\n",
      "Epoch 9/100\n",
      "2605/2605 [==============================] - 2s 938us/step - loss: 2.6646e-04\n",
      "Epoch 10/100\n",
      "2605/2605 [==============================] - 2s 941us/step - loss: 2.5746e-04\n",
      "Epoch 11/100\n",
      "2605/2605 [==============================] - 2s 955us/step - loss: 2.5423e-04\n",
      "Epoch 12/100\n",
      "2605/2605 [==============================] - 2s 957us/step - loss: 2.4545e-04\n",
      "Epoch 13/100\n",
      "2605/2605 [==============================] - 2s 935us/step - loss: 2.4939e-04\n",
      "Epoch 14/100\n",
      "2605/2605 [==============================] - 2s 943us/step - loss: 2.5059e-04\n",
      "Epoch 15/100\n",
      "2605/2605 [==============================] - 2s 944us/step - loss: 2.4258e-04\n",
      "Epoch 16/100\n",
      "2605/2605 [==============================] - 2s 945us/step - loss: 2.4330e-04\n",
      "Epoch 17/100\n",
      "2605/2605 [==============================] - 2s 943us/step - loss: 2.4024e-04\n",
      "Epoch 18/100\n",
      "2605/2605 [==============================] - 3s 981us/step - loss: 2.3873e-04\n",
      "Epoch 19/100\n",
      "2605/2605 [==============================] - 2s 950us/step - loss: 2.3681e-04\n",
      "Epoch 20/100\n",
      "2605/2605 [==============================] - 2s 934us/step - loss: 2.3681e-04\n",
      "Epoch 21/100\n",
      "2605/2605 [==============================] - 2s 943us/step - loss: 2.3145e-04\n",
      "Epoch 22/100\n",
      "2605/2605 [==============================] - 2s 944us/step - loss: 2.3204e-04\n",
      "Epoch 23/100\n",
      "2605/2605 [==============================] - 2s 946us/step - loss: 2.2836e-04\n",
      "Epoch 24/100\n",
      "2605/2605 [==============================] - 3s 971us/step - loss: 2.4419e-04\n",
      "Epoch 25/100\n",
      "2605/2605 [==============================] - 3s 969us/step - loss: 2.3814e-04\n",
      "Epoch 26/100\n",
      "2605/2605 [==============================] - 2s 945us/step - loss: 2.5855e-04\n",
      "Epoch 27/100\n",
      "2605/2605 [==============================] - 2s 941us/step - loss: 2.4503e-04\n",
      "Epoch 28/100\n",
      "2605/2605 [==============================] - 3s 969us/step - loss: 2.4082e-04\n",
      "Epoch 29/100\n",
      "2605/2605 [==============================] - 2s 952us/step - loss: 2.3521e-04\n",
      "Epoch 30/100\n",
      "2605/2605 [==============================] - 2s 942us/step - loss: 2.3658e-04\n",
      "Epoch 31/100\n",
      "2605/2605 [==============================] - 3s 982us/step - loss: 2.3204e-04\n",
      "Epoch 32/100\n",
      "2605/2605 [==============================] - 2s 947us/step - loss: 2.3006e-04\n",
      "Epoch 33/100\n",
      "2605/2605 [==============================] - 2s 951us/step - loss: 2.3194e-04\n",
      "Epoch 34/100\n",
      "2605/2605 [==============================] - 2s 947us/step - loss: 2.2841e-04\n",
      "Epoch 35/100\n",
      "2605/2605 [==============================] - 2s 948us/step - loss: 2.2765e-04\n",
      "Epoch 36/100\n",
      "2605/2605 [==============================] - 2s 943us/step - loss: 2.2715e-04\n",
      "Epoch 37/100\n",
      "2605/2605 [==============================] - 3s 966us/step - loss: 2.2690e-04\n",
      "Epoch 38/100\n",
      "2605/2605 [==============================] - 3s 960us/step - loss: 2.2348e-04\n",
      "Epoch 39/100\n",
      "2605/2605 [==============================] - 2s 943us/step - loss: 2.2494e-04\n",
      "Epoch 40/100\n",
      "2605/2605 [==============================] - 2s 953us/step - loss: 2.2324e-04\n",
      "Epoch 41/100\n",
      "2605/2605 [==============================] - 3s 960us/step - loss: 2.2423e-04\n",
      "Epoch 42/100\n",
      "2605/2605 [==============================] - 2s 945us/step - loss: 2.2101e-04\n",
      "Epoch 43/100\n",
      "2605/2605 [==============================] - 2s 959us/step - loss: 2.1983e-04\n",
      "Epoch 44/100\n",
      "2605/2605 [==============================] - 2s 959us/step - loss: 2.1982e-04\n",
      "Epoch 45/100\n",
      "2605/2605 [==============================] - 2s 937us/step - loss: 2.2266e-04\n",
      "Epoch 46/100\n",
      "2605/2605 [==============================] - 2s 947us/step - loss: 2.1874e-04\n",
      "Epoch 47/100\n",
      "2605/2605 [==============================] - 2s 941us/step - loss: 2.1952e-04\n",
      "Epoch 48/100\n",
      "2605/2605 [==============================] - 2s 939us/step - loss: 2.1821e-04\n",
      "Epoch 49/100\n",
      "2605/2605 [==============================] - 2s 946us/step - loss: 2.1800e-04\n",
      "Epoch 50/100\n",
      "2605/2605 [==============================] - 3s 962us/step - loss: 2.1712e-04\n",
      "Epoch 51/100\n",
      "2605/2605 [==============================] - 2s 951us/step - loss: 2.1849e-04\n",
      "Epoch 52/100\n",
      "2605/2605 [==============================] - 2s 949us/step - loss: 2.1635e-04\n",
      "Epoch 53/100\n",
      "2605/2605 [==============================] - 2s 943us/step - loss: 2.1770e-04\n",
      "Epoch 54/100\n",
      "2605/2605 [==============================] - 2s 945us/step - loss: 2.1655e-04\n",
      "Epoch 55/100\n",
      "2605/2605 [==============================] - 2s 949us/step - loss: 2.1623e-04\n",
      "Epoch 56/100\n",
      "2605/2605 [==============================] - 2s 948us/step - loss: 2.1356e-04\n",
      "Epoch 57/100\n",
      "2605/2605 [==============================] - 3s 971us/step - loss: 2.1353e-04\n",
      "Epoch 58/100\n",
      "2605/2605 [==============================] - 2s 947us/step - loss: 2.1444e-04\n",
      "Epoch 59/100\n",
      "2605/2605 [==============================] - 2s 944us/step - loss: 2.1589e-04\n",
      "Epoch 60/100\n",
      "2605/2605 [==============================] - 2s 954us/step - loss: 2.1448e-04\n",
      "Epoch 61/100\n",
      "2605/2605 [==============================] - 2s 945us/step - loss: 2.1273e-04\n",
      "Epoch 62/100\n",
      "2605/2605 [==============================] - 2s 946us/step - loss: 2.1677e-04\n",
      "Epoch 63/100\n",
      "2605/2605 [==============================] - 3s 986us/step - loss: 2.1072e-04\n",
      "Epoch 64/100\n",
      "2605/2605 [==============================] - 2s 947us/step - loss: 2.1363e-04\n",
      "Epoch 65/100\n",
      "2605/2605 [==============================] - 2s 937us/step - loss: 2.1341e-04\n",
      "Epoch 66/100\n",
      "2605/2605 [==============================] - 2s 952us/step - loss: 2.0977e-04\n",
      "Epoch 67/100\n",
      "2605/2605 [==============================] - 2s 942us/step - loss: 2.1112e-04\n",
      "Epoch 68/100\n",
      "2605/2605 [==============================] - 2s 942us/step - loss: 2.1331e-04\n",
      "Epoch 69/100\n",
      "2605/2605 [==============================] - 3s 966us/step - loss: 2.1252e-04\n",
      "Epoch 70/100\n",
      "2605/2605 [==============================] - 3s 971us/step - loss: 2.1169e-04\n",
      "Epoch 71/100\n",
      "2605/2605 [==============================] - 2s 953us/step - loss: 2.1013e-04\n",
      "Epoch 72/100\n",
      "2605/2605 [==============================] - 2s 945us/step - loss: 2.1157e-04\n",
      "Epoch 73/100\n",
      "2605/2605 [==============================] - 2s 948us/step - loss: 2.1235e-04\n",
      "Epoch 74/100\n",
      "2605/2605 [==============================] - 2s 950us/step - loss: 2.0847e-04\n",
      "Epoch 75/100\n",
      "2605/2605 [==============================] - 2s 952us/step - loss: 2.1017e-04\n",
      "Epoch 76/100\n",
      "2605/2605 [==============================] - 3s 973us/step - loss: 2.1132e-04\n",
      "Epoch 77/100\n",
      "2605/2605 [==============================] - 3s 963us/step - loss: 2.0846e-04\n",
      "Epoch 78/100\n",
      "2605/2605 [==============================] - 2s 945us/step - loss: 2.0905e-04\n",
      "Epoch 79/100\n",
      "2605/2605 [==============================] - 2s 951us/step - loss: 2.0993e-04\n",
      "Epoch 80/100\n",
      "2605/2605 [==============================] - 2s 946us/step - loss: 2.0879e-04\n",
      "Epoch 81/100\n",
      "2605/2605 [==============================] - 2s 938us/step - loss: 2.0667e-04\n",
      "Epoch 82/100\n",
      "2605/2605 [==============================] - 3s 971us/step - loss: 2.0799e-04\n",
      "Epoch 83/100\n",
      "2605/2605 [==============================] - 3s 962us/step - loss: 2.0892e-04\n",
      "Epoch 84/100\n",
      "2605/2605 [==============================] - 2s 952us/step - loss: 2.0764e-04\n",
      "Epoch 85/100\n",
      "2605/2605 [==============================] - 2s 938us/step - loss: 2.0716e-04\n",
      "Epoch 86/100\n",
      "2605/2605 [==============================] - 2s 939us/step - loss: 2.0428e-04\n",
      "Epoch 87/100\n",
      "2605/2605 [==============================] - 2s 950us/step - loss: 2.0568e-04\n",
      "Epoch 88/100\n",
      "2605/2605 [==============================] - 2s 945us/step - loss: 2.0668e-04\n",
      "Epoch 89/100\n",
      "2605/2605 [==============================] - 3s 988us/step - loss: 2.0489e-04\n",
      "Epoch 90/100\n",
      "2605/2605 [==============================] - 2s 944us/step - loss: 2.0385e-04\n",
      "Epoch 91/100\n",
      "2605/2605 [==============================] - 2s 948us/step - loss: 2.0673e-04\n",
      "Epoch 92/100\n",
      "2605/2605 [==============================] - 2s 946us/step - loss: 2.0724e-04\n",
      "Epoch 93/100\n",
      "2605/2605 [==============================] - 2s 946us/step - loss: 2.0537e-04\n",
      "Epoch 94/100\n",
      "2605/2605 [==============================] - 2s 951us/step - loss: 2.0713e-04\n",
      "Epoch 95/100\n",
      "2605/2605 [==============================] - 3s 960us/step - loss: 2.0547e-04\n",
      "Epoch 96/100\n",
      "2605/2605 [==============================] - 2s 958us/step - loss: 2.0482e-04\n",
      "Epoch 97/100\n",
      "2605/2605 [==============================] - 2s 946us/step - loss: 2.0620e-04\n",
      "Epoch 98/100\n",
      "2605/2605 [==============================] - 2s 941us/step - loss: 2.0215e-04\n",
      "Epoch 99/100\n",
      "2605/2605 [==============================] - 2s 951us/step - loss: 2.0335e-04\n",
      "Epoch 100/100\n",
      "2605/2605 [==============================] - 2s 946us/step - loss: 2.0473e-04\n",
      "2605/2605 [==============================] - 2s 700us/step\n",
      "La mean square error és: [0.00019977]\n",
      "La mean absolute error és[0.00019977]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "modelNN = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Una salida\n",
    "])\n",
    " #Entrenament de la xarxa neuronal\n",
    "modelNN.compile(optimizer='adam', loss='mean_squared_error')\n",
    "modelNN.fit(x_train, y_train, epochs=100, batch_size=32)\n",
    "y_pred = modelNN.predict(x_test)\n",
    "\n",
    "print(\"La mean square error és: \" + str(check_precision_MSE(y_pred, y_test)))\n",
    "print(\"La mean absolute error és\" + str(check_precision_MSE(y_pred, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
